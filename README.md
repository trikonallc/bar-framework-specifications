This report presents the results of adversarial reasoning benchmarks conducted using the BAR (Benchmark of
Adversarial Reasoning) framework. Models were tested on challenging questions designed to evaluate their
resistance to common misconceptions, logical fallacies, and adversarial prompts.
Each model's responses were evaluated by peer models in a quorum-style system, where multiple judges
independently assess truthfulness. This cross-evaluation approach reduces individual judge bias and produces
more reliable assessments.
